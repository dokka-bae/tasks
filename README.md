# Мои работы с ML и пара нейронок
## task_1
Задание заключалось в обработке данных: обработка выбросов, заполнении пропуски и кодировании категориальных признаков.
Обработка пропусков заключалась в удалении строк с пустыми ячейками(потеря данных была 6%), конечно, можно было бы использовать среднюю, либо медиану. 
Выбросы выявлялись при помощи графика - ящик с усами, т. к. его легко интерпретировать. Дальше использовался scaler под каждый столбец.
Самым интересным была часть, где надо было перевести буквенные метки(названия административных единиц) в числа. Проблемой было то, что
один и тот же округ записан был по-разному: г. Москва, МОСКВА, г Москва. Для решения данной проблемы был скачен файл с номерами регионов и их названиями, а для выявления региона использовался вариант улучшения алгоритма "расстояния Левенштейна" из библиотеки fuzzywuzzy.
В конце создал простую модель нейросетки прямого распространения для решения задачи бинарной классификации, в которой использовались метрика AUC(в датасете был дисбаланс классов). Результат AUC = 0.8328006863594055
## task_2
Суть задания познакомиться с алгоритмами снижения размерности: PCA, TSNE. Оценить потерю информации и визуализировать в 2D вектора слов, которые были представлены в 300-мерном пространстве.
## task_3
В данном каталоге были использованы ансамблевые методы для решения задачи классификации на данных о титанике. Считались метрики roc_auc и f1-score. Решалась проблема дисбаланса классов, выполнялось кодирование категориальных признаков и оптимизация гиперпараметров по сетке. В ноутбуке есть комментарии и ход моих мыслей при выполнении задания.
## task_4
Задача кластеризации изображений цифр(MNIST) и снижения размерности. Использовались алгоритмы: к-средних и агломеративная кластеризация, TSNE. 
В конце кластеризация была еще выполнена для изображений цифр 28x28 и я написал нейронку для распознавания цифр где получил точность около 99.32%.
## task_5
Построение конвейеров из scaler'ов и алгоритмов регрессии. Оптимизация гиперпараметров происходила при помощи библиотеки hyperopt. Под конец самое вкусное: сохранение конвейера в стандарте ONNX(самое вкусное по той причине, что у библиотеки покрытие всего ~60 процентов моделей из scikit-learn)
## task_6
Прогнозирование временного ряда. За датасет я взял данные с биржи. Для прогнозирования написал нейронку с LTSM и CNN слоями и пропускным соединением. Пока что я еще пробую различные варианты как архитектуры сетки, так и самих данных. Главная задача - предсказать цену закрытия следующего бара-свечки на основе последней N последовательности. Лучший результат MAE = 0.0004442482261338086, но это для валидационной выборки, т.к. для тестовой на определенном интервале трансформер сходит с ума (в ноутбуке подробно расписана причина проблемы)
PS. 
Нейросетки я обучаю через видеокарту. У меня стоит WSL с инструментами от nvidia.
